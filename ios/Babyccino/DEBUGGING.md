# Debugging Guide

## Overview

Comprehensive logging has been added throughout the conversation flow to make debugging easier. All logs use a consistent format with emojis and prefixes for easy filtering.

## Log Format

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ” [Component] Function name called
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¬ [Component] Specific action description
ğŸ“ [Component] Data/state information
âœ… [Component] Success message
âŒ [Component] Error message
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

## Components with Logging

### 1. ChatViewModel (ChatView.swift)

**Entry points:**
- `sendMessage()` - User sends a message
- `getAssistantResponse()` - Getting LLM response
- `generateCode()` - Code generation flow

**Example output:**
```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¬ [ChatViewModel] sendMessage() called
ğŸ“ [ChatViewModel] User message: "I want a prime checker"
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š [ChatViewModel] Conversation now has 2 messages
ğŸ’­ [ChatViewModel] Getting assistant response
```

### 2. MLXLLMService (MLXLLMService.swift)

**Key functions:**
- `generateResponse()` - Main LLM inference
- `extractRequirements()` - Pattern matching for function requirements
- `extractRequirementsWithPatternMatching()` - Detailed pattern matching

**Example output:**
```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ” [MLXLLMService] generateResponse() called
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“ [MLXLLMService] Conversation has 2 messages
ğŸ’¬ [MLXLLMService] Last user message: "I want a prime checker"
ğŸ¯ [MLXLLMService] No special command detected, proceeding with LLM generation
ğŸ“‹ [MLXLLMService] Built ChatML prompt (450 chars)
ğŸ“‹ [MLXLLMService] Prompt preview (first 200 chars):
   <|im_start|>system
You are a coordinator...
ğŸ¤– [MLXLLMService] Starting MLX inference...
âœ… [MLXLLMService] Generated 85 characters
ğŸ“„ [MLXLLMService] Raw model output:
   "I can help with that. Say 'generate code' when ready."
âœ“ [MLXLLMService] No filtering needed
âœ“ [MLXLLMService] Final response: "I can help with that. Say 'generate code' when ready."
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### 3. Requirements Extraction

**Pattern matching logs:**
```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ” [MLXLLMService] extractRequirements() called
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“ [MLXLLMService] Conversation history (3 messages):
   [0] user: I want a prime checker
   [1] assistant: I can help with that. Say 'generate code' when ready.
   [2] user: generate code
ğŸ’­ [MLXLLMService] Using pattern matching extraction (LLM extraction TODO)
ğŸ” [MLXLLMService] extractRequirementsWithPatternMatching() started
ğŸ” [MLXLLMService] Examining last 2 user messages:
   [0] I want a prime checker
   [1] generate code
ğŸ” [MLXLLMService] Checking message 0 (reversed): "generate code"
âœ— [MLXLLMService] No pattern matched in this message
ğŸ” [MLXLLMService] Checking message 1 (reversed): "I want a prime checker"
âœ“ [MLXLLMService] Matched pattern: PRIME
ğŸ¯ [MLXLLMService] Found match, stopping search
ğŸ“‹ [MLXLLMService] Extracted requirements:
   Name: is_prime
   Purpose: Check if a number is prime
   Parameters: 1
   Return Type: bool
   Edge Cases: 3
   Examples: 4
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

## Filtering Logs

To see specific components:

```bash
# View all MLXLLMService logs
xcrun simctl spawn booted log stream --predicate 'eventMessage CONTAINS "[MLXLLMService]"'

# View all ChatViewModel logs
xcrun simctl spawn booted log stream --predicate 'eventMessage CONTAINS "[ChatViewModel]"'

# View only errors
xcrun simctl spawn booted log stream --predicate 'eventMessage CONTAINS "âŒ"'

# View pattern matching
xcrun simctl spawn booted log stream --predicate 'eventMessage CONTAINS "Matched pattern"'
```

Or simply look at Xcode console output during debugging.

## Common Debugging Scenarios

### Scenario 1: LLM Not Responding Correctly

**Check these logs:**
1. `[ChatViewModel] User message` - Verify what was sent
2. `[MLXLLMService] Last user message` - Confirm it was received
3. `[MLXLLMService] Raw model output` - See what model generated
4. `[MLXLLMService] Cleaned output` - See what was filtered
5. `[MLXLLMService] Final response` - See what was returned

**Example debug flow:**
```
User says: "how about a prime checker?"

Look for:
ğŸ’¬ [ChatViewModel] User message: "how about a prime checker?"
â†’ âœ… Message received

ğŸ’¬ [MLXLLMService] Last user message: "how about a prime checker?"
â†’ âœ… Passed to LLM

ğŸ“„ [MLXLLMService] Raw model output: "Sure, I can help..."
â†’ âŒ Model didn't follow instructions

ğŸ§¹ [MLXLLMService] Filtering applied - removed 120 characters
â†’ âœ… Filtering working

âœ“ [MLXLLMService] Final response: "I can help with that. Say 'generate code' when ready."
â†’ âœ… Correct output after filtering
```

### Scenario 2: Wrong Function Generated

**Check these logs:**
1. `[MLXLLMService] Conversation history` - Review full conversation
2. `[MLXLLMService] Examining last N user messages` - See what's being analyzed
3. `[MLXLLMService] Matched pattern: X` - See which pattern matched
4. `[MLXLLMService] Extracted requirements` - Verify extracted details

**Example debug flow:**
```
User first asks for palindrome, then prime.
Code generator returns palindrome function.

Look for:
ğŸ“ [MLXLLMService] Conversation history (4 messages):
   [0] user: I want a palindrome checker
   [1] assistant: I can help with that...
   [2] user: how about a prime checker?
   [3] assistant: I can help with that...
â†’ âœ… Both requests in history

ğŸ” [MLXLLMService] Examining last 2 user messages:
   [0] I want a palindrome checker
   [1] how about a prime checker?
â†’ âœ… Both messages being checked

ğŸ” [MLXLLMService] Checking message 0 (reversed): "how about a prime checker?"
âœ“ [MLXLLMService] Matched pattern: PRIME
â†’ âœ… Should extract prime, not palindrome

ğŸ“‹ [MLXLLMService] Extracted requirements:
   Name: is_prime
â†’ âœ… Correct extraction
```

### Scenario 3: Special Commands Not Detected

**Check these logs:**
1. `[ChatViewModel] User message` - See exact input
2. `[MLXLLMService] Special command detected` - Check detection
3. If no detection, check `[MLXLLMService] proceeding with LLM generation`

**Example debug flow:**
```
User says "generate code" but nothing happens

Look for:
ğŸ’¬ [ChatViewModel] User message: "generate code"
ğŸ¯ [ChatViewModel] Detected GENERATE CODE command
â†’ âœ… Detected in ChatViewModel

ğŸ¯ [MLXLLMService] Special command detected: GENERATE_CODE
â†’ âœ… Also detected in service

ğŸ”§ [ChatViewModel] generateCode() called
â†’ âœ… Function called

ğŸ“‹ [ChatViewModel] Extracted 1 function requirement(s)
   [0] is_prime: Check if a number is prime
â†’ âœ… Requirements extracted

ğŸŒ [ChatViewModel] Calling serverClient.generateCode()
â†’ Check if server call succeeds
```

## Log Emoji Key

- ğŸ” Function entry / inspection
- ğŸ’¬ User message / communication
- ğŸ“ Data / state information
- ğŸ“‹ Structured data (lists, requirements)
- ğŸ¯ Special command detected / important decision
- ğŸ’­ Thinking / processing step
- ğŸ”„ Using a service / calling function
- âœ… Success / confirmation
- âœ“ Minor success / validation passed
- âŒ Error / failure
- âš ï¸ Warning / fallback used
- ğŸ¤– AI/LLM operation
- ğŸ§¹ Filtering / cleanup
- ğŸ“„ Raw data / output
- ğŸ”§ Code generation
- ğŸŒ Network call
- ğŸ“Š Statistics / counts
- ğŸ“± Platform detection

## Tips

1. **Start from the top** - Follow logs chronologically to see the full flow
2. **Look for breaks** - The `â”â”â”` lines separate different function calls
3. **Check reversals** - Pattern matching processes messages in reverse order
4. **Verify filtering** - Compare "Raw model output" vs "Final response"
5. **Count messages** - Ensure conversation history is correct length
6. **Check platform** - Simulator vs physical device affects which code runs

## Future Improvements

- [ ] Add timestamp to logs
- [ ] Add request ID to track conversations
- [ ] Log inference timing (tokens/sec)
- [ ] Add log level filtering (DEBUG, INFO, ERROR)
- [ ] Export logs to file for bug reports
